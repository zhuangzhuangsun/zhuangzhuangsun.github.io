<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<title>
  
    Archive: 2019
  
</title>

<meta property="og:type" content="website">
<meta property="og:title" content="韭菜熟了">
<meta property="og:url" content="http://yoursite.com/archives/2019/index.html">
<meta property="og:site_name" content="韭菜熟了">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="韭菜熟了">


  <link rel="alternative" href="/atom.xml" title="韭菜熟了" type="application/atom+xml">



  <link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="/perfect-scrollbar/css/perfect-scrollbar.min.css">
<link rel="stylesheet" href="/styles/main.css">






</head>
<body>
  <div class="mobile-header">
  <button class="sidebar-toggle" type="button">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
  </button>
  <a class="title" href="/">韭菜熟了</a>
</div>

  <div class="main-container">
    <div class="sidebar">
  <div class="header">
    <h1 class="title"><a href="/">韭菜熟了</a></h1>
    
    <div class="info">
      <div class="content">
        
        
          <div class="author">mike</div>
        
      </div>
      
        <div class="avatar">
          
            <a href="www.zzzszzz.xyz"><img src="https://i.loli.net/2019/04/17/5cb6ab0c2a104.jpeg"></a>
          
        </div>
      
    </div>
  </div>
  <div class="body">
    
      
        <ul class="nav">
          
            
              <li class="category-list-container">
                <a href="javascript:;">Category</a>
                <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/algorithm/">algorithm</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/computer-system/">computer system</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/diary/">diary</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/program-lauguage/">program lauguage</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><span class="category-list-count">1</span></li></ul>
              </li>
            
          
            
              <li class="tag-list-container">
                <a href="javascript:;">Tag</a>
                <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">-hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/炮姐-动漫/">-炮姐 -动漫</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phash/">Phash</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SAR/">SAR</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dhash/">dhash</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/matalb/">matalb</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/matlab/">matlab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/superpixel/">superpixel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/">tensorflow</a><span class="tag-list-count">9</span></li></ul>
              </li>
            
          
            
              <li class="archive-list-container">
                <a href="javascript:;">Archive</a>
                <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">30</span></li></ul>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="/" title="Homepage">Homepage</a>
              </li>
            
          
            
              <li>
                <a href="/archives" title="By Year">By Year</a>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="https://github.com/denjones/hexo-theme-chan" title="Chan" target="_blank" rel="noopener">Chan</a>
              </li>
            
          
            
              <li>
                <a href="https://github.com/zhuangzhuangsun" title="Github" target="_blank" rel="noopener">Github</a>
              </li>
            
          
            
              <li>
                <a href="/atom.xml" title="RSS">RSS</a>
              </li>
            
          
        </ul>
      
    
  </div>
</div>

    <div class="main-content">
      
        <div style="max-width: 1000px">
      
          

  
  
    
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <h1><a href="/archives/2019" class="archive-year">2019</a></h1>
        </div>
        <div class="post-list">
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard-tf-app-run/">
  tensorboard_tf.app.run
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard-tf-app-run/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h3 id="tf-app-flags"><a href="#tf-app-flags" class="headerlink" title="tf.app.flags"></a>tf.app.flags</h3><p>tf.app.flags主要用于处理命令行参数的解析工作，其实可以理解为一个封装好了的argparse包（argparse是一种结构化的数据存储格式，类似于Json、XML）。</p>
<p>在tensorflow中我们该怎么使用呢？<br>首先我们通过<code>tf.app.flags</code>来调用这个<code>flags.py</code>文件，这样我们就可以用<code>flags.DEFINE_interger/float()</code>来添加命令行参数，而<code>FLAGS=flags.FLAGS</code>可以实例化这个解析参数的类从对应的命令行参数取出参数。<br>新建test.py文件，并输入如下代码，代码的功能是创建几个命令行参数，然后把命令行参数输出显示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(<span class="string">'data_dir'</span>, <span class="string">'/tmp/mnist'</span>, <span class="string">'Directory with the MNIST data.'</span>)</span><br><span class="line">flags.DEFINE_integer(<span class="string">'batch_size'</span>, <span class="number">5</span>, <span class="string">'Batch size.'</span>)</span><br><span class="line">flags.DEFINE_integer(<span class="string">'num_evals'</span>, <span class="number">1000</span>, <span class="string">'Number of batches to evaluate.'</span>)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line">print(FLAGS.data_dir, FLAGS.batch_size, FLAGS.num_evals)</span><br></pre></td></tr></table></figure>
<ul>
<li>在命令行中输入<code>test.py -h</code>就可以查看帮助信息，也就是<code>Directory with the MNIST data.</code>，<code>Batch size</code>和<code>Number of batches to evaluate</code>这样的消息。</li>
<li>在命令行中输入<code>test.py --batchsize 10</code>就可以将batch_size的值修改为10！</li>
</ul>
<h3 id="tf-app-run"><a href="#tf-app-run" class="headerlink" title="tf.app.run()"></a>tf.app.run()</h3><p>该函数一般都是出现在这种代码中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p>上述第一行代码表示如果当前是从其它模块调用的该模块程序，则不会运行main函数！而如果就是直接运行的该模块程序，则会运行main函数。</p>
<p>具体第二行的功能从源码开始分析，源码如下：</p>
<p><img src="E:\markdown\image\tf.app.run(" alt="tf.app.run()">.png)</p>
<p><code>flags_passthrough=f._parse_flags(args=args)</code>这里的<code>parse_flags</code>就是我们<code>tf.app.flags</code>源码中用来解析命令行参数的函数。所以这一行就是解析参数的功能；</p>
<p>下面两行代码也就是<code>tf.app.run</code>的核心意思：<strong>执行程序中main函数，并解析命令行参数！</strong></p>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard队列和线程/">
  tensorboard队列和线程
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard队列和线程/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="队列和线程"><a href="#队列和线程" class="headerlink" title="队列和线程"></a>队列和线程</h1><h3 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h3><p>Tensorflow中的队列(queue)本身也是图中的一个节点，是一种有状态的节点，其他节点，如入队节点(enqueue)和出队节点(dequeue)都可以修改它的内容。入队节点可以把新元素插入到队列末尾，出队节点可以把队列前面的元素删除。</p>
<p>Tensorflow中主要有两种队列，即<strong>FIFOQueue</strong>和<strong>RandomShuffleQueue</strong>。</p>
<ol>
<li><p><strong>FIFOQueue</strong></p>
<p>FIFOQueue创建一个先入先出队列，当我们希望读入的训练样本为有序的，例如语音，文字样本。可以使用FIFOQueue。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个FIFOQueue  初始化队列插入0.1，0.2，0.3三个数字</span></span><br><span class="line">q=tf.FIFOQueue(<span class="number">3</span>,<span class="string">'float'</span>)</span><br><span class="line">init=q.enqueue_many(([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>],))</span><br><span class="line"><span class="comment">#定义出队，+1，入队操作  即将x出队加一后再入队</span></span><br><span class="line">x=q.dequeue()</span><br><span class="line">y=x+<span class="number">1</span></span><br><span class="line">q_inc=q.enqueue([y])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启一个Session 执行两次q_inc操作  随后查看队列q的内容</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        sess.run(q_inc)</span><br><span class="line">        quelen=sess.run(q.size())</span><br><span class="line">    fro i <span class="keyword">in</span> range(quelen):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输出为：</span><br><span class="line"><span class="number">0.3</span></span><br><span class="line"><span class="number">1.1</span></span><br><span class="line"><span class="number">1.2</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="2">
<li><p><strong>RandomShuffleQueue</strong></p>
<p><strong>RandomShuffleQueue</strong>创建一个随机队列，在出队列时，以随机的顺序产生元素，例如，在训练一些图像样本时，使用CNN，希望可以无序地读入训练样本，就可以使用<strong>RandomShuffleQueue</strong>,每次随机产生一个训练样本。</p>
<p><strong>RandomShuffleQueue在Tensorflow使用异步计算时非常重要，因为Tensorflow会话是支持多线程的，可以在主线程里执行训练操作，使用RandomShuffleQueue作为训练输入，开多个线程来准备训练样本</strong></p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#创建一个RandomShuffleQueue，队列最大长度为10  出队后最小长度为2</span></span><br><span class="line">q=tf.RandomShuffleQueue(capacity=<span class="number">10</span>,min_after_dequeue=<span class="number">2</span>,dtypes=<span class="string">'float'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启一个Session 执行两次q_inc操作  随后查看队列q的内容</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        sess.run(q.enqueue(i))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">输出为：</span><br><span class="line"><span class="number">6.0</span></span><br><span class="line"><span class="number">7.0</span></span><br><span class="line"><span class="number">2.0</span></span><br><span class="line"><span class="number">4.0</span></span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<p>​        我们尝试修改入队次数为12次，再次运行，发现程序阻断不动，或者我们尝试修改出队次数为10次，即不保留队列最小长度，发现队列输出八次结果后，在终端仍然阻断了，在箭头处卡住不动，称为阻断。</p>
<p>阻断一般发生在：</p>
<ul>
<li><p>队列长度等于最小值时，执行出队操作</p>
</li>
<li><p>对列长度等于最大值时，执行入队操作</p>
<p>只有当队列满足要求后，才能继续执行，可以通过设置会话在运行时得等待时间来解除阻断</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">run_options=tf.RunOptions(timeout_in_ms=<span class="number">10000</span>)<span class="comment">#等待10s</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    sess.run(q.dequeue(),options=run_options)</span><br><span class="line"><span class="keyword">except</span> tf.errors.DeadlineExceededError:</span><br><span class="line">    print(<span class="string">'out of range'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="队列管理器-QueueRunner"><a href="#队列管理器-QueueRunner" class="headerlink" title="队列管理器(QueueRunner)"></a>队列管理器(QueueRunner)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先创建一个含有队列的图</span></span><br><span class="line">q=tf.FIFOQueue(<span class="number">1000</span>,<span class="string">'float'</span>)</span><br><span class="line">counter=tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">increment_op=tf.assign_add(counter,tf.constant(<span class="number">1.0</span>))<span class="comment">#操作给counter加一</span></span><br><span class="line">enqueue_op=q.enqueue([counter])</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个队列管理器QueueRunner，用这两个操作向队列q中添加元素，只使用一个线程</span></span><br><span class="line">qr=tf.train.QueueRunner(q,enqueue_ops=[increment_op,enqueue_op]*<span class="number">1</span>)</span><br><span class="line"><span class="comment">#启动一个会话，从队列管理器qr中创建线程</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variable_initializer())</span><br><span class="line">    enqueue_threads=qr.create_threads(sess,start=true)<span class="comment">#启动入队线程</span></span><br><span class="line">    <span class="comment">#主线程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br><span class="line">        </span><br><span class="line"><span class="comment">#输出为：</span></span><br><span class="line"></span><br><span class="line"><span class="number">2.0</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">4.0</span></span><br><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="number">6.0</span></span><br><span class="line"><span class="number">7.0</span></span><br><span class="line"><span class="number">7.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">10.0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>输出并不是我们所期待的自然序列，并且线程被阻断，这是因为加一操作与入队操作不同步，可能加一操作执行了许多次之后，才会进行一次入队操作，另外因为主线程的训练(出队操作)和兑取数据的线程的训练(入队操作)时异步的，主线程会一直等待数据送入。</p>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard矩阵常用操作/">
  tensorboard矩阵常用操作
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard矩阵常用操作/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="tensorflow矩阵运算"><a href="#tensorflow矩阵运算" class="headerlink" title="tensorflow矩阵运算"></a>tensorflow矩阵运算</h1><h3 id="点乘"><a href="#点乘" class="headerlink" title="点乘"></a>点乘</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对应元素相乘</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(a*b))</span><br><span class="line"><span class="comment">#或者sess.run(tf.multiply(a,b))</span></span><br></pre></td></tr></table></figure>
<h3 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">print(sess.run(tf.matmul(c,d)))</span><br></pre></td></tr></table></figure>
<h3 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵和向量相加，每列加上向量</span></span><br><span class="line">e = tf.constant([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">print(sess.run(tf.matmul(c,d)+e))</span><br></pre></td></tr></table></figure>
<h3 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.transpose()</span><br><span class="line">transpose(</span><br><span class="line">    a,</span><br><span class="line">    perm=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">'transpose'</span></span><br><span class="line">)<span class="comment">#参数表</span></span><br><span class="line"><span class="comment">#a为待转置的矩阵  perm为一个List，例如[0，2，1]代表第一维度和第二维度交换</span></span><br></pre></td></tr></table></figure>
<h3 id="改变形状"><a href="#改变形状" class="headerlink" title="改变形状"></a>改变形状</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape()</span><br><span class="line"><span class="comment">#sample</span></span><br><span class="line">x=tf.get_variable(<span class="string">'wc7_s'</span>, shape=[<span class="number">512</span>,<span class="number">256</span>],initializer=init_glorot_uniform)</span><br><span class="line">x=tf.reshape(x,[<span class="number">1</span>,<span class="number">512</span>,<span class="number">-1</span>])</span><br><span class="line"><span class="comment">#-1代表自适应</span></span><br></pre></td></tr></table></figure>
<h3 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.concat()</span><br><span class="line"><span class="comment">#例子</span></span><br><span class="line">concact_1=tf.concat([uppool_1,maxpool5],<span class="number">3</span>)</span><br><span class="line"><span class="comment">#代表将uppool_1和maxpool5在第三维度上拼接</span></span><br></pre></td></tr></table></figure>
<h3 id="某个维度上复制"><a href="#某个维度上复制" class="headerlink" title="某个维度上复制"></a>某个维度上复制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.tile()</span><br><span class="line">map=tf.get_variable(<span class="string">'map'</span>,shape=[<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">map_copy=tf.tile(map,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment">#作用为将map在第3个通道上复制6遍  其余通道保持不变  map_copy的shape=[5,4,3,12]</span></span><br></pre></td></tr></table></figure>
<h3 id="转换数据类型"><a href="#转换数据类型" class="headerlink" title="转换数据类型"></a>转换数据类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.cast()</span><br><span class="line"><span class="comment">#例子</span></span><br><span class="line">a=tf.Variable(<span class="number">5</span>,name=<span class="string">'a'</span>,dtype=tf.float32)</span><br><span class="line">tf.cast(a,tf.float64)</span><br></pre></td></tr></table></figure>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard模型保存与加载/">
  tensorboard模型保存与加载
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard模型保存与加载/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="tensorflow模型的保存与加载"><a href="#tensorflow模型的保存与加载" class="headerlink" title="tensorflow模型的保存与加载"></a>tensorflow模型的保存与加载</h1><h3 id="模型文件"><a href="#模型文件" class="headerlink" title="模型文件"></a>模型文件</h3><h5 id="A）元数据图（meta-graph）："><a href="#A）元数据图（meta-graph）：" class="headerlink" title="A）元数据图（meta graph）："></a>A）元数据图（meta graph）：</h5><p>它保存了tensorflow完整的网络图结构。这个文件以<strong>.META</strong>为扩展名。</p>
<h5 id="B）检查点文件（checkpoint-file）"><a href="#B）检查点文件（checkpoint-file）" class="headerlink" title="B）检查点文件（checkpoint file）"></a>B）检查点文件（checkpoint file）</h5><p>这是一个二进制文件，它包含的权重变量，biases变量和其他变量。有两个文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mymodel.data<span class="number">-00000</span>-of<span class="number">-00001</span></span><br><span class="line">mymodel.index</span><br></pre></td></tr></table></figure>
<h5 id="C）检查点（checkpoint）"><a href="#C）检查点（checkpoint）" class="headerlink" title="C）检查点（checkpoint）"></a>C）检查点（checkpoint）</h5><p>checkpoint文件保存了一个目录下所有的模型文件列表，这个文件是tf.train.Saver类自动生成且自动维护的。在 checkpoint文件中维护了由一个tf.train.Saver类持久化的所有TensorFlow模型文件的文件名。当某个保存的TensorFlow模型文件被删除时，这个模型所对应的文件名也会从checkpoint文件中删除。</p>
<h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><p>在训练模型的时候，需要一直关注着模型损失值和模型准确度。一旦你发现，网络已经收敛，就可以手动停止训练。训练完成后，我们希望将所有的变量和网络模型保存下来，供以后使用。在Tensorflow中要保存所有这些，使用tf.train.Saver（）来保存神经网络的网络结构图和相关变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
<p>Tensorflow变量的作用范围是在一个session里面。在保存模型的时候，应该在session里面通过save方法保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, <span class="string">'my-test-model'</span>)</span><br></pre></td></tr></table></figure>
<p>在这里，sess是session对象，而“my-test-model”是模型的名称</p>
<p>如果我们希望在迭代1000次之后保存模型，可以把当前的迭代步数传进去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, <span class="string">'my_test_model'</span>,global_step=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>这样“-1000”将追加在文件名中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_test_model<span class="number">-1000.</span>index</span><br><span class="line">my_test_model<span class="number">-1000.</span>meta</span><br><span class="line">my_test_model<span class="number">-1000.</span>data<span class="number">-00000</span>-of<span class="number">-00001</span></span><br><span class="line">checkpoint</span><br></pre></td></tr></table></figure>
<p>在训练的时候，假设每1000次就保存一次模型，但是，这些保存的文件中变化的只是神经网络的变量，而网络的结构是没有变化的，所以就没有必要重复保存.meta文件，可以使用下面的方式，只让网络结构保存一次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, <span class="string">'my-model'</span>, global_step=step,write_meta_graph=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>如果你想只保留最新的4个模型，并希望每2小时保存一次，可以使用max_to_keep和keep_checkpoint_every_n_hours。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#saves a model every 2 hours and maximum 4 latest models are saved.</span></span><br><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">4</span>, keep_checkpoint_every_n_hours=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>请注意，如果我们没有在tf.train.Saver（）指定任何参数，这样就表示要保存所有的变量</strong>。如果，我们不希望保存所有的变量，只是其中的一部分。我们可以指定要保存的变量或者集合。怎么做？在创建tf.train.Saver的时候我们把一个列表或者要保存变量的字典作为参数传进去。让我们来看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w1 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>]), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal(shape=[<span class="number">5</span>]), name=<span class="string">'w2'</span>)</span><br><span class="line">saver = tf.train.Saver([w1,w2])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">saver.save(sess, <span class="string">'my_test_model'</span>,global_step=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>这用于保存Tensorflow图的特定部分。</p>
<h3 id="模型恢复"><a href="#模型恢复" class="headerlink" title="模型恢复"></a>模型恢复</h3><p>ensorflow中有operation和tensor，前者表示 操作 ，后者表示 容器 ，每个operation都是有一个tensor来存放值的，比如y=f(x), operation是f(x), tensor存放的就是y，如果要获取y，就必须输入x<br>tensor的名字一般是 <operation>:<num></num></operation></p>
<p>可以通过 print(out.name) 来看看</p>
<p>假如之前的训练定义了如下图（模型），并保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">bottom = layers.fully_connected(inputs=bottom, num_outputs=<span class="number">7</span>, activation_fn=<span class="literal">None</span>, scope=<span class="string">'logits_classifier'</span>)</span><br><span class="line">......</span><br><span class="line">prediction = tf.nn.softmax(logits, name=<span class="string">'prob'</span>)</span><br><span class="line">......</span><br><span class="line">saver_path = <span class="string">'./model/checkpoint/model.ckpt'</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth=<span class="literal">True</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">......</span><br><span class="line">	saved_path = saver.save(sess,saver_path) <span class="comment"># 这个保存了三个东西， .meta是图的结构， 还有两个是模型中变量的值</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>要想图结构和模型（恢复图结构，没错，从空白的代码段中恢复一个graph，就不需要重新定义图了）    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">meta_path = <span class="string">'./model/checkpoint/model.ckpt.meta'</span></span><br><span class="line">model_path = <span class="string">'./model/checkpoint/model.ckpt'</span></span><br><span class="line">saver = tf.train.import_meta_graph(meta_path) <span class="comment"># 导入图</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth=<span class="literal">True</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, model_path) <span class="comment"># 导入变量值</span></span><br><span class="line">    graph = tf.get_default_graph()</span><br><span class="line">    prob_op = graph.get_operation_by_name(<span class="string">'prob'</span>)</span><br><span class="line">    <span class="comment"># 这个只是获取了operation， 至于有什么用还不知道</span></span><br><span class="line">    prediction = graph.get_tensor_by_name(<span class="string">'prob:0'</span>)</span><br><span class="line">    <span class="comment"># 获取之前prob那个操作的输出，即prediction</span></span><br><span class="line">    print( ress.run(prediciton, feed_dict=&#123;...&#125;)) </span><br><span class="line">	print(sess.run(graph.get_tensor_by_name(<span class="string">'logits_classifier/weights:0'</span>)))</span><br></pre></td></tr></table></figure>
<p>关于获取保存的模型中的tensor或者输出，还有一种办法就是用tf.add_to_collection()，<br>假如上面每次定义一次运算后，可以在后面添加tf.add_to_collection()：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bottom = layers.fully_connected(inputs=bottom, num_outputs=<span class="number">7</span>, activation_fn=<span class="literal">None</span>, scope=<span class="string">'logits_classifier'</span>)</span><br><span class="line"><span class="comment">### add collection</span></span><br><span class="line">tf.add_to_collection(<span class="string">'logits'</span>,bottom)</span><br><span class="line">......</span><br><span class="line">prediction = tf.nn.softmax(logits, name=<span class="string">'prob'</span>)</span><br><span class="line"><span class="comment">### add collection</span></span><br><span class="line">tf.add_to_collection(<span class="string">'prob'</span>,prediction)</span><br></pre></td></tr></table></figure>
<p>恢复模型后，通过tf.get_collection()来获取tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">x = tf.get_collection(<span class="string">'inputs'</span>)[<span class="number">0</span>]</span><br><span class="line">prob = tf.get_collection(<span class="string">'prob'</span>)[<span class="number">0</span>]</span><br><span class="line">print(x)</span><br><span class="line">print(prob)</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<p>可以查看输出，效果是和上面get_tensor_by_name()一样的，注意get_collection(name)的name只是collection的name，tensor的名字还是原来的名字</p>
<p>得到了模型各个地方的tensor之后，要想获取该地方的参数或者输出的值，只需要通过sess.run()就可以了，参数可以直接run，中间的特征或者预测值需要通过feed_dict={}传递输入的值就行啦</p>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard数据加载/">
  tensorboard数据加载
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard数据加载/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="Tensorflow加载数据"><a href="#Tensorflow加载数据" class="headerlink" title="Tensorflow加载数据"></a>Tensorflow加载数据</h1><p>Tensorflow有三种读取数据的方法</p>
<ul>
<li>预加载数据(preloaded data)。即在tensorflow图中定义常量或者变量来保存数据</li>
<li>填充数据(feeding)。python产生数据，再将数据填充到后端。</li>
<li>从文件中读取(reading from file)。从文件中直接读取，让队列管理器从文件中读取数据</li>
</ul>
<h3 id="预加载数据"><a href="#预加载数据" class="headerlink" title="预加载数据"></a>预加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x1=tf.constant([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x2=tf.constant([<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">y=tf.add(x1,x2)</span><br></pre></td></tr></table></figure>
<p>此方法缺点为将数据直接嵌入数据流图中，当数据较大时，比较消耗内存</p>
<h3 id="填充数据"><a href="#填充数据" class="headerlink" title="填充数据"></a>填充数据</h3><p>使用sess.run()中的feed_dict参数，将python产生的数据填充入后端</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a1=tf.placeholder(tf.int16)</span><br><span class="line">a2=tf.placeholder(tf.int16)</span><br><span class="line">b=tf.add(a1,a2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据</span></span><br><span class="line"></span><br><span class="line">li1=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">li2=[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(b,feed_dict=&#123;a1:li1,a2:li2&#125;))</span><br></pre></td></tr></table></figure>
<p>此方法也有数据量大，消耗内存等缺点，并且数据类型转换等中间环节增加了不小的开销</p>
<h3 id="从文件读取数据"><a href="#从文件读取数据" class="headerlink" title="从文件读取数据"></a>从文件读取数据</h3><h5 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h5><ol>
<li><p>把样本数据写入TFRecords二进制文件</p>
</li>
<li><p>从队列中读取数据</p>
<p><strong>TFRecords</strong>是一种二进制文件，能更好地利用内存，更方便的复制和移动，并且不需要单独的标记文件。</p>
</li>
</ol>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard控制流操作/">
  tensorboard控制流操作
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard控制流操作/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> i  = <span class="number">0</span></span><br><span class="line"> n =<span class="number">10</span> </span><br><span class="line"><span class="comment">#循环条件函数</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">cond</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> i &lt; n</span><br><span class="line"><span class="comment">#循环主体函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> i, n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#[i,n]为循环输入的初始值</span></span><br><span class="line">i, n = tf.while_loop(cond, body, [i, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tip:</span></span><br><span class="line"><span class="string">cond()函数的输入与body（）函数应该与循环输入初始值变量一一对应 此例中为[i,n]</span></span><br><span class="line"><span class="string">body（）函数的输出应与循环输入初始变量一致   此例中为[i,n]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n=tf.constant(np.max(superpixelmap),dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">loss=tf.Variable(<span class="number">0.0</span>,name=<span class="string">'consistency_loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cond</span><span class="params">(i,loss,result,map)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> i&lt;n</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i,loss,result,map)</span>:</span></span><br><span class="line">    loss+=backend.a_superpixel_consistency_loss(result,map,i)</span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span>,loss,result,map</span><br><span class="line">i,loss,result,map=tf.while_loop(cond,body,(<span class="number">0</span>,loss,result_student,superpixelmap))</span><br></pre></td></tr></table></figure>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard张量定义及初始化/">
  tensorboard张量定义及初始化
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard张量定义及初始化/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="张量定义及初始化"><a href="#张量定义及初始化" class="headerlink" title="张量定义及初始化"></a>张量定义及初始化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, </span><br><span class="line">caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, </span><br><span class="line">import_scope=None)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(name, shape=<span class="literal">None</span>, dtype=<span class="literal">None</span>, initializer=<span class="literal">None</span>, regularizer=<span class="literal">None</span>, </span><br><span class="line">trainable=<span class="literal">True</span>, collections=<span class="literal">None</span>, caching_device=<span class="literal">None</span>, partitioner=<span class="literal">None</span>, validate_shape=<span class="literal">True</span>, </span><br><span class="line">custom_getter=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>使用<code>tf.Variable</code>时，如果检测到命名冲突，系统会自己处理。使用<code>tf.get_variable()</code>时，系统不会处理冲突，而会报错</li>
<li><code>tf.Variable()</code> 每次都在创建新对象，所有<code>reuse=True</code> 和它并没有什么关系。对于<code>get_variable()</code>，来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#常量定义</span></span><br><span class="line">D = tf.zeros([<span class="number">2</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">E = tf.ones([<span class="number">2</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">F = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br></pre></td></tr></table></figure>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard命名空间/">
  tensorboard命名空间
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard命名空间/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <p>TensorFlow的命名空间分为两种，tf.variable_scope和tf.name_scope。</p>
<h4 id="tf-variable-scope"><a href="#tf-variable-scope" class="headerlink" title="tf.variable_scope"></a>tf.variable_scope</h4><p>当使用tf.get_variable定义变量时，如果出现同名的情况将会引起报错</p>
<p>而对于tf.Variable来说，却可以定义“同名”变量，但是把这些图变量的name属性打印出来，就可以发现它们的名称并不是一样的。</p>
<p>如果想使用tf.get_variable来定义另一个同名图变量，可以考虑加入新一层scope，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">In [<span class="number">2</span>]: <span class="keyword">with</span> tf.variable_scope(<span class="string">'scope1'</span>):</span><br><span class="line">   ...:     v1 = tf.get_variable(<span class="string">'var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">   ...:     <span class="keyword">with</span> tf.variable_scope(<span class="string">'scope2'</span>):</span><br><span class="line">   ...:         v2 = tf.get_variable(<span class="string">'var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">   ...:</span><br><span class="line">In [<span class="number">3</span>]: v1.name, v2.name</span><br><span class="line">Out[<span class="number">3</span>]: (<span class="string">'scope1/var:0'</span>, <span class="string">'scope1/scope2/var:0'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf-name-scope"><a href="#tf-name-scope" class="headerlink" title="tf.name_scope"></a>tf.name_scope</h4><p>当tf.get_variable遇上tf.name_scope，它定义的变量的最终完整名称将不受这个tf.name_scope的影响，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">In [<span class="number">2</span>]: <span class="keyword">with</span> tf.variable_scope(<span class="string">'v_scope'</span>):</span><br><span class="line">   ...:     <span class="keyword">with</span> tf.name_scope(<span class="string">'n_scope'</span>):</span><br><span class="line">   ...:         x = tf.Variable([<span class="number">1</span>], name=<span class="string">'x'</span>)</span><br><span class="line">   ...:         y = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">1</span>], dtype=tf.int32)</span><br><span class="line">   ...:         z = x + y</span><br><span class="line">   ...:</span><br><span class="line">In [<span class="number">3</span>]: x.name, y.name, z.name</span><br><span class="line">Out[<span class="number">3</span>]: (<span class="string">'v_scope/n_scope/x:0'</span>, <span class="string">'v_scope/x:0'</span>, <span class="string">'v_scope/n_scope/add:0'</span>)</span><br></pre></td></tr></table></figure>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/07/29/tensorboard使用/">
  tensorboard使用
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/07/29/tensorboard使用/"><span class="article-date">
  2019-07-29
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/tensorflow/">tensorflow</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <h1 id="TensorBoard的使用"><a href="#TensorBoard的使用" class="headerlink" title="TensorBoard的使用"></a>TensorBoard的使用</h1><p>使用TensorBoard展示数据，需要在执行Tensorflow计算图的过程中，将各种类型的数据（<code>summary protobuf</code>）汇总并记录到日志文件中。然后使用TensorBoard读取这些日志文件，解析数据并生产数据可视化的Web页面，让我们可以在浏览器中观察各种汇总数据。</p>
<h4 id="tensorboard可视化的数据类型"><a href="#tensorboard可视化的数据类型" class="headerlink" title="tensorboard可视化的数据类型"></a>tensorboard可视化的数据类型</h4><ol>
<li><strong>SCALARS</strong>，对标量数据进行汇总和记录<br>使用方法：<code>tf.summary.scalar(tags, values, collections=None, name=None)</code></li>
<li><strong>IMAGES</strong>， 汇总数据中的图像，例如MNIST中可以将输入的向量还原成图片的像素矩阵<br>使用方法：<code>tf.summary.image(tag, tensor, max_images=3, collections=None, name=None)</code></li>
<li><strong>GRAPHS</strong>， 可视化Tensorflow计算图的结构及计算图上的信息<br>使用方法：<code>tf.summary.FileWriter(logdir, graph)</code><br>其实这个方法是将当前summary protobuf写近日志文件中，但是会自动生成计算图。</li>
<li><strong>HISTOGRAMS</strong>，记录变量的直方图(张量中元素的取值分布）<br>使用方法：<code>tf.summary.histogram(tag, values, collections=None, name=None）</code></li>
</ol>
<h5 id="merge-all"><a href="#merge-all" class="headerlink" title="merge_all()"></a>merge_all()</h5><p>和Tensorflow类似，<code>tf.summaru.histograms()</code>等函数不会立即执行，需要通过<code>sess.run()</code>来明确调用，当日志程序中定义写日志的操作比较多时，可以使用<code>summ = tf.summary.merge_all()</code>函数来整理所有的日志生成操作，最后只需要<code>sess.run(summ)</code>即可将定义中的所有日志生成操作一次执行。</p>
<h5 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h5><ol>
<li>添加记录节点：<code>tf.summary.scalar/image/histogram()</code>等</li>
<li>汇总记录节点：<code>merged = tf.summary.merge_all()</code> </li>
<li>运行汇总节点：<code>summary = sess.run(merged)</code>，得到汇总结果</li>
<li>日志书写器实例化：<code>summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)</code>，实例化的同时传入 graph 将当前计算图写入日志</li>
<li>调用日志书写器实例对象<code>summary_writer</code>的<code>add_summary(summary, global_step=i)</code>方法将所有汇总日志写入文件</li>
<li>调用日志书写器实例对象<code>summary_writer</code>的<code>close()</code>方法写入内存，否则它每隔120s写入一次</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义命名空间</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'input'</span>):</span><br><span class="line">  <span class="comment">#fetch：就是同时运行多个op的意思</span></span><br><span class="line">  input1 = tf.constant(<span class="number">3.0</span>,name=<span class="string">'A'</span>)<span class="comment">#定义名称，会在tensorboard中代替显示</span></span><br><span class="line">  input2 = tf.constant(<span class="number">4.0</span>,name=<span class="string">'B'</span>)</span><br><span class="line">  input3 = tf.constant(<span class="number">5.0</span>,name=<span class="string">'C'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'op'</span>):</span><br><span class="line">  <span class="comment">#加法</span></span><br><span class="line">  add = tf.add(input2,input3)</span><br><span class="line">  <span class="comment">#乘法</span></span><br><span class="line">  mul = tf.multiply(input1,add)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment">#默认在当前py目录下的logs文件夹，没有会自己创建</span></span><br><span class="line">  wirter = tf.summary.FileWriter(<span class="string">'logs/'</span>,sess.graph)</span><br><span class="line">  result = ss.run([mul,add])</span><br><span class="line">  print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=路径</span><br></pre></td></tr></table></figure>
<p>在浏览器中使用<code>http://DESKTOP-JGL4HV5:6006</code>查看可视化结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">## prepare the original data</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'data'</span>):</span><br><span class="line">     x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line">     y_data = <span class="number">0.3</span>*x_data+<span class="number">0.1</span></span><br><span class="line"><span class="comment">##creat parameters</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'parameters'</span>):</span><br><span class="line">     <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">            weight = tf.Variable(tf.random_uniform([<span class="number">1</span>],<span class="number">-1.0</span>,<span class="number">1.0</span>))</span><br><span class="line">           tf.summary.histogram(<span class="string">'weight'</span>,weight)</span><br><span class="line">     <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">           bias = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">           tf.summary.histogram(<span class="string">'bias'</span>,bias)</span><br><span class="line"><span class="comment">##get y_prediction</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'y_prediction'</span>):</span><br><span class="line">     y_prediction = weight*x_data+bias</span><br><span class="line"><span class="comment">##compute the loss</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">     loss = tf.reduce_mean(tf.square(y_data-y_prediction))</span><br><span class="line">     tf.summary.scalar(<span class="string">'loss'</span>,loss)</span><br><span class="line"><span class="comment">##creat optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#creat train ,minimize the loss </span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">     train = optimizer.minimize(loss)</span><br><span class="line"><span class="comment">#creat init</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'init'</span>): </span><br><span class="line">     init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment">##creat a Session </span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment">#merged</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"><span class="comment">##initialize</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br><span class="line">sess.run(init)</span><br><span class="line"><span class="comment">## Loop</span></span><br><span class="line"><span class="keyword">for</span> step  <span class="keyword">in</span>  range(<span class="number">101</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    rs=sess.run(merged)</span><br><span class="line">    writer.add_summary(rs, step)</span><br></pre></td></tr></table></figure>

        
      </div>
    </div>

  

  
  
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/06/13/关于python的引用/">
  关于python的引用
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/06/13/关于python的引用/"><span class="article-date">
  2019-06-13
</span>
</a>
        
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/program-lauguage/">program lauguage</a></li></ul>
	</span>


        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <ol>
<li>python不允许程序员选择传值还是传引用。python参数传递参用的肯定是“传对象引用”的方式。如果函数收到的是一个可变对象(字典或者列表)的引用，就可以修改对象的原始值，若为不可变对象(如元组或者字符)，则不可直接修改对象。</li>
<li>当复制列表或者字典时，就复制了对象列表的引用，如果改变引用的值，则修改了原始的参数。</li>
<li>为了简化内存管理，Python通过引用计数器的方式实现垃圾自动回收。Python对每个对象都有一个引用计数，每当其被引用一次，计数值就会加一，每销毁一次Python对象，计数值就会减一，当计数值为零时，才从内存中删除Python对象。
          <p>
            <a href="/2019/06/13/关于python的引用/">
              Read More
            </a>
          </p>
        
      </li></ol></div>
    </div>

  



  <div class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/archives/2019/page/2/">2</a><a class="page-number" href="/archives/2019/page/3/">3</a><a class="extend next" rel="next" href="/archives/2019/page/2/">Next</a>
  </div>


</div></section>

          <div class="main-footer">
  
    © 2019 韭菜熟了 - Powered by <a href="http://hexo.io" target="_blank">Hexo</a> - Theme <a href="https://github.com/denjones/hexo-theme-chan" target="_blank">Chan</a>
  
</div>

      
        </div>
      
    </div>
  </div>
  <script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

  <link rel="stylesheet" href="/PhotoSwipe/photoswipe.css">
  <link rel="stylesheet" href="/PhotoSwipe/default-skin/default-skin.css">

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
  <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
             It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

      <!-- Container that holds slides.
                PhotoSwipe keeps only 3 of them in the DOM to save memory.
                Don't modify these 3 pswp__item elements, data is added later on. -->
      <div class="pswp__container">
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
      </div>

      <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
      <div class="pswp__ui pswp__ui--hidden">

        <div class="pswp__top-bar">

          <!--  Controls are self-explanatory. Order can be changed. -->

          <div class="pswp__counter"></div>

          <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

          <button class="pswp__button pswp__button--share" title="Share"></button>

          <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

          <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

          <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
          <!-- element will get class pswp__preloader--active when preloader is running -->
          <div class="pswp__preloader">
            <div class="pswp__preloader__icn">
              <div class="pswp__preloader__cut">
                <div class="pswp__preloader__donut"></div>
              </div>
            </div>
          </div>
        </div>

        <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
          <div class="pswp__share-tooltip"></div>
        </div>

        <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>

        <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

        <div class="pswp__caption">
          <div class="pswp__caption__center"></div>
        </div>
      </div>
    </div>
  </div>

  <script src="/PhotoSwipe/photoswipe.js"></script>
  <script src="/PhotoSwipe/photoswipe-ui-default.js"></script>


<script src="/perfect-scrollbar/js/min/perfect-scrollbar.min.js"></script>
<script src="/scripts/main.js"></script>

</body>
</html>
